\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{minibox}
\usepackage{verbatim}
% \usepackage{pdf14} % Enable for Manuscriptcentral -- can't handle pdf 1.5
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some
% submissions.

\usepackage[
    natbib=true,
    bibencoding=inputenc,
    bibstyle=authoryear-ibid,
    citestyle=authoryear-comp,
    maxcitenames=3,
    maxbibnames=10,
    useprefix=false,
    sortcites=true,
    backend=biber
]{biblatex}
\AtBeginDocument{\toggletrue{blx@useprefix}}
\AtBeginBibliography{\togglefalse{blx@useprefix}}
\setlength{\bibitemsep}{1.5ex}
\addbibresource{refs.bib}

\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=NavyBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=NavyBlue
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}


\begin{document}

\title{Revision of: Forecasting Economic Activity using a Text-Classification Model\thanks{Fabian Schmidt, University of Bonn. Email: \href{mailto:schmidt.fabian-christian@t-online.de}{\nolinkurl{schmidt [dot] fabian-christian [at] t-online [dot] de}}.}}

\author{Fabian Schmidt}

\date{
    \today
}

\maketitle


\begin{abstract}
    With this project, I am trying to improve forecasts of economic activity in the short-run by adding a measure of sentiment to the analysis. To do so, I first webscraped a thousand headlines from the German newspaper website "welt.de" and then analyzed them using a text-classification model.
    I then hand-labelled a few of the headlines and trained the model on these hand-labelled headlines. I then used this model to again analyze the headlines and compare the results. Lastly, the sentiment labels are used in a VAR model to forecast economic activity in the short-run.\\
    The results are as follows: Using sentiment values derived from a text-classification model did not improve forecasting performance in the short-term in general. The main issue in forecast performance seems to be the ability of the models to forecast the sentiment values themselves rather than the values belonging to the economic indicator. Nevertheless, there are some results that are promising that the inclusion of a sentiment component could lead to a slight improvement in forecasting performance in the very short-term.
\end{abstract}

\clearpage


\section{Introduction} % (fold)
\label{sec:introduction}

With this project, I am trying to improve forecasts of economic activity in the short-run using a text-classification or more specifically sentiment analysis model using newspaper headlines. The goal of this project is to see if a text-classification model can improve forecasts from standard econometric models (e.g. autoregressive models) in the short-run by adding a measure of sentiment to the analysis.
A secondary goal is to analyze how much data is needed, what kind of data is most useful and which model one should pick, and how much tuning needs to be done to forecast short-run economic activity using newspaper headlines.
This template is using the econ-project-templates from \citet{Gaudecker2023}.
The rest of the paper is structured as follows:
\begin{enumerate}
    \item The idea behind the project will be swiftly explained and similar research that influenced the choice of the project will be mentioned and briefly summarized.
    \item The setup of the project will be documented. Datasets and the issue with getting this data will be discussed. The used model and methods to analyze the sentiment of the headlines will be presented.
    \item The results of this project will be presented and discussed.
    \item Extensions and improvements of the project will be discussed.
\end{enumerate}
Since this is the second version of this project, I will also highlight some of the changes that were made to this project that changed the outcome of the analysis.

% section introduction (end)
\section{The idea behind the project}
In this section, I will discuss the idea behind this project as well as mention a couple of similar research projects that were influential in my choice of this project. This section has by no means the ambition to provide a complete overview of all the literature similar to this project, but is rather a history of thought behind the project.

The idea behind this project is to see if text-classification machine learning models can improve forecasting of economic relevant variables. Such an improvement is desirable since an improvement in forecasts can lead to better information availability and, therefore, improve decision-making by firms, households, institutions, and governments. The traditional approach to forecasting macroeconomic variables usually features auto-regressive models that try to put traditional economic variables like inflation or the unemployment rate into relation to each other as well as into relation to each other's lags and exogenous shocks. Alternatively, dynamic stochastic general equilibrium (DSGE) models are used to forecast macroeconomic variables by capturing the intricate interplay of economic agents in different market environments, enabling a more comprehensive understanding of how various factors influence the trajectory of key economic indicators. Both of these approaches, however, can struggle to make precise short-term forecasts for macroeconomic variables. For example, during the start of the COVID-19 crisis forecasting models were not able to correctly identify the magnitude of the exogenous shock hitting the economy resulting in the ECB having to re-evaluate their forecasting models \parencite{Battistini2021}. Furthermore, in the aftermath of the COVID-19 pandemic forecasting models were, again, unable to correctly predict the rise in inflation and the duration of how long inflation would stay at this high level. One possible explanation for why this might be the case is that expectations of the general public influence the short-term behaviour of these variables in ways that are challenging to capture for these models.
In recent history, machine learning models were introduced to forecasting with the idea that large deep learning models might be able to infer insights from the data that standard auto-regressive models could not find. One strand of the literature also looked at designing more subjective measurements and using these for forecasting economically relevant variables. For example, \textcite{Denes2022} used Twitter data to measure inflation perception using a random forest. They show that their indicator is consistent with measured inflation perception by household surveys. They also show that their indicator is strongly correlated with the actual inflation rate which could spark the idea that such a model could be used for forecasting inflation in the short-term since inflation measurements are usually not readily accessible on a day-to-day basis, and sometimes they aren't even updated monthly.

Another study is taking a similar approach. The Finance and Economics Discussion Series (FEDS) working paper by \textcite{Adams_2023} uses the FinBert model to analyze the sentiment of Twitter data to then use this sentiment indicator to predict next-day stock market returns. The study finds that sentiment derived from Twitter data contains predictive information for stock returns and reacts to monetary policy surprises, anticipating tightening moves before Federal Open Market Committee statement releases.

My approach is building on this literature. However, rather than focusing on a specific group like Twitter users, I try to find an approach that resembles the sentiment of the overall population. Additionally, I do not focus on the financial market, but rather on real economic activity.

Potential research questions for this project, thus, are: Can text-classification machine learning models improve forecasts for real economic activity? Are newspaper articles or headlines a good indicator of current political, economic, and financial developments and the sentiment related to those developments? Which models can be used to do sentiment analysis? How much data is needed to train such a model?
I will try to answer some of these questions in this project. These questions can then be, potentially, further investigated in follow-up work like, for example, a master's thesis.

\section{The Setup of this Project}

To do this project, I need two sources of data. One indicator for economic activity and one source of data that is representative of the public sentiment about current political, economic, and financial developments inside of the economy. I decided to focus on one specific country since trying to analyze the sentiment of the entire world using text-classification models is most likely not possible and most definitely too big of a task for this project. Since I am from Germany and I am doing this project at a German university, I decided to focus on the German economy and the German population.

As a source of data to analyze public sentiment, I chose to use newspaper headlines. That is because, firstly, a lot of German citizens tend to read classical newspapers. An evaluation of the survey \textcite{Gesellschaft2022} found that about 80 percent of the German population above 14 years old read newspapers either physically or online regularly. As a consequence, it could be expected that newspapers could potentially influence the sentiment of the general public on current developments and, secondly, it could be expected that journalists and newspaper agencies have a good understanding of the overall public sentiment of the population that is reflected in their articles as well. Therefore, I decided to choose newspaper headlines as an indicator of overall public sentiment.

I focused on headlines rather than whole newspaper articles since, firstly, usually the headline of an article is sufficient to analyze the overall sentiment of the article. Secondly, whole newspaper articles are usually not open-accessible without paying for access or receiving access through an institution which was not possible in my case. Furthermore, analyzing whole newspaper articles would most likely make the process that is used for sentiment analysis done by machine learning models more difficult since more inputs per headline would most likely also require more data to tune the model with. Additionally, this amount of data could be difficult for soft- and hardware to deal with. This would have been most definitely the case for the soft- and hardware that I was working with. Therefore, I decided to focus on headlines rather than on whole newspaper articles.

Since newspaper headlines are not openly accessible in a public database, but rather can be found on the websites of newspapers, I decided to webscrape them from the website of the newspaper. I chose the newspaper "Die Welt" as the representative newspaper for Germany since the newspaper is one of the biggest daily newspapers in Germany and it has all of their past headlines openly accessible on their websites making it much easier to webscrape them. An additional benefit from choosing "Die Welt"'s website to webscrape the data from is that they have categorized their newspaper articles into easily distinguishable categories. Therefore, it was easy for me to only choose newspaper headlines that were located in the right category so that I would not analyze headlines that are not reflective of the general public's current sentiment. As part of the revision of this project I revisited this part of the project since I was not happy with the code that was used for webscraping in the previous version of this project. I found that investigating the web-application programming interface (API) of the specific website lead to the use of more fitted webscraping methods. After investigating the API of the website from the German newspaper "Die Welt" I found that using a webscraper like Selenium that is able to interact with JavaScript elements like buttons, forms, etc. was not suited to my specific application since the ability of interacting with such elements leads to a loss in efficiency and simplicity. As a consequence, I switched the webscraping framework from Selenium to BeautifulSoup, a powerful library for parsing HTML and XML documents. I also examined the option of getting the data directly from the API, however, this was sadly not possible in my specific example. The code for the webscraping part can be found in the source code of the project's GitHub repository.

As the second source of data, I needed an indicator of economic activity in Germany. As criteria for choosing this indicator, I tried to find an indicator that was available daily and was representative of real economic activity in Germany. The latter I chose as criteria because I try to analyze the overall public sentiment of the population in Germany. Therefore, only focusing on specific markets would not be coherent with the overall idea of forecasting overall economic activity with sentiment analysis.
I chose to use the daily truck toll mileage indicator issued by the German Bundesbank and the German Federal Statistical Office \parencite{Destatis2024}. This indicator is a relatively new indicator for economic activity and, therefore, parts of it are still experimental. The index is supposed to provide approximate indications of the development of industrial production in Germany at an early stage. The index developed by the Federal Office for Goods Transport traces the development of the mileage of large trucks (with four or more axles) on German motorways. It is calculated from digital process data of the truck toll collection system. The daily data of the truck toll mileage index are also published in calendar and seasonally adjusted form; the adjustment is carried out by the German Bundesbank. As the methods of seasonal adjustment of daily data are still under development, the seasonally adjusted daily data of the truck toll mileage index are described as experimental.

I chose this specific indicator since it is available daily and it provides a new source for measuring economic activity in Germany that has not been used a lot, yet.

Before using this indicator, I had to detrend the indicator since the focus of my analysis was on analyzing if newspaper headlines can help to better detect short-term developments. Therefore, I wanted to remove the long-run trend from the data. I did so by applying the Hodrick-Prescott filter. The Hodrick-Prescott filter is a statistical filter specifically developed for finding the trend and cycle component of macroeconomic variables from the data. As a consequence, the filter was perfectly suited for my specific task.

The other components that I needed for my project were a sentiment analysis model for analyzing newspaper headlines and multiple forecasting models for analyzing if the analyzed sentiments could improve forecasting performance. Since I needed an encoder-only model for analyzing sentiments, a Bert model was an obvious choice. I decided to use the multilingual-sentiment-newspaper-headlines model by Zachary Dickson which is accessible via huggingface. This model is a fine-tuned version of the bert-base-multilingual-cased model that was fine-tuned on a dataset of 30 thousand newspaper headlines in German, Polish, English, Dutch, and Spanish. The dataset contains six thousand headlines in each of the five languages.
One benefit in choosing this specific model is that this model was already partly trained on newspaper headlines from the newspaper "Die Welt" as documented on the model's webpage.

The other model components that I needed were multiple forecasting models for forecasting economic activity in Germany partly using the previously created sentiment indicator. For forecasting, I chose two more traditional models. The two traditional forecasting models that I chose were an ARMA model as a baseline model and a VAR model as the model using the sentiment indicator previously created.

\section{Results}

In the next section, I will present the results of the project. The results of the project can be summarized like the following: Using newspaper-analyzed sentiment labels did not improve forecasting economic activity as a whole. There is some indication that forecasting economic activity might improve in the very short-run (1-4 days) using sentiment labels. The main issue with using sentiment labels for forecasting economic activity seems to be that to forecast multiple periods into the future one also has to forecast the sentiment labels themselves into the future which I did not manage to do very precisely. Therefore, forecasting accuracy seems to get drastically more imprecise after a couple of days. Additional issues with the sentiment indicator were that while the sentiment analysis of single headlines was relatively precise and I was even able to improve the performance slightly by fine-tuning the model on a small hand-labelled dataset, the model was not able to analyze the sentiment of all headlines on one specific day. This was the case due to a limitation of input embeddings in the configuration of the model. However, even when using a smaller set of headlines and trying to analyze them all together at once, the model still did relatively poorly at analyzing the sentiment of these headlines. As a consequence, I had to analyze each headline separately and then took the mean over all the sentiment labels of that day. This led to the sentiment labels being rather imprecise when being compared to a small hand-labelled dataset due to there not being any weighting for all the headlines of a specific day. Possible improvements to the labelling process and how that could affect the forecasting results will also be discussed here.

I will now present some of the results of the project in detail and finish this section by discussing possible improvements and extensions.

Firstly, I will analyze the results of the sentiment labelling of the newspaper headlines. To do so I will first analyze the classification report and confusion matrix of the test data for the zero-shot classification model and then do the same for the fine-tuned model. I will then compare their results and talk about the implications of these results. As the dataset for the evaluation, I used 300 hand-labelled headlines that I labelled myself. While I tried to remove personal biases and opinions when labelling the dataset my opinion and the sentiment that I sort to certain headlines is, of course, also just subjective and the scores that I will present here should be viewed under this consideration.

\input{../tables/classification_report_zero_shot_classification_model.tex}



\input{../tables/confusion_matrix_zero_shot_classification_model.tex}

Above, one can see the classification report and confusion matrix of the zero-shot classification done by the sentiment analysis model. Here, the value "0" corresponds to a "negative" label while the value "1" corresponds to a "neutral" label and the value "2" corresponds to a "positive" label.  As we can see the macro and weighted precision of the zero-shot classification is quite similar to the macro and weighted recall score. The recall accuracy of the label "negative" exceeds the precision accuracy while the precision score of the label "neutral" exceeds the recall score of that label. From the confusion matrix, one can also infer this result. Here, headlines that were "neutral" labelled were often classified as "negative" leading to the low precision score for the "negative" label and the low recall score for the "neutral" label. Surprisingly, the model was quite good at labelling headlines that had a "positive" sentiment. In the previous version of this project, this was the main weakness of the zero-shot classification model. The improvement in that regard, however, is random since besides the headlines that were picked to be in the test set, no changes were made elsewhere to cause this result.

At next, I will discuss the results of the fine-tuned version of the model. As the dataset for the fine-tuning I, again, used the 300 hand-labelled headlines. 200 of those were randomly selected as training data while 50 were selected as evaluation and 50 as test data. The metric that I used was the macro f1-score. I decided to use the macro f1-score rather than the weighted f1-score since my data is slightly imbalanced. I decided to go for the f1-score since I could not find any reason why one would prefer the precision or the recall score in my specific scenario.
I decided to only use one training epoch for my model since when I experimented with different amounts of training epochs I found that the model was overfitting the data after one training epoch. Therefore, I only chose one.

\input{../tables/classification_report_finetuned_model.tex}



\input{../tables/confusion_matrix_finetuned_model.tex}

Above, we can see the classification report and confusion matrix of the fine-tuned model. As one is able to recognize almost all scores have improved due to fine-tuning. Especially the precision score of the model has improved significantly while the recall score has only improved slightly and even got worse slightly for the macro-score. The only slight improvement in the recall score is mostly due to a significantly worse recall score for the label "negative" and the label "positive". The recall score still improved since, at the same time, the recall score for the label "neutral" improved significantly and has more support than the recall score for the label "negative" and "positive" combined.
Looking at the confusion matrix, we can identify that all of the "negative" labelled headlines that were incorrectly identified were identified as "neutral". The same can be said for headlines that were labelled "positive". This is a positive development from the confusion matrix of the zero-shot classification model since this means that while labels are still often times incorrectly identified they are at least not identified as their direct opposites.

At next, I will discuss the results of the forecasting part of this project.

For forecasting economic activity I decided to use two traditional time-series forecasting models, an ARMA model and a VAR model. I used the ARMA model as a baseline model while the VAR model featured the newly created sentiment variable and where, therefore, compared to the baseline model. As an improvement to the previous version of this project, I carefully selected a specific time period instead of choosing one arbitrarily. I made sure that the calculated cycle values in the time span that I picked where stationary and the time span did not include any dates in the first week of January and any days in December since upon further investigation these time periods seem to be the ones were the values are the most volatile. I also chose a more recent time span than in the previous version of the project. The time period that I picked was from the seventh of January, 2023 to the thirtieth of November, 2023.

\input{../tables/summary_statistics_ARIMA_model.tex}

In the table above, one can see the summary statistics of the ARMA model. As we can see the lag-order that was chosen by the model is an ARMA model with two auto-regressive lags and one moving-average component. All coefficient values of the model are significant at a significance level of five percent with the largest coefficient value being the one of the first auto-regressive component.

\VerbatimInput{../tables/summary_statistics_VAR_model.txt}

In the next table above, we can observe the summary statistics of the VAR model. For the VAR model, I chose the Akaike Information Criterion (AIC) as the criterion to select the amount of lags included. The AIC selected a model with one auto-regressive lag. As we can see for this model only two of all possible coefficients are significant. The two coefficients are the cycle values auto-regressive component of itself and the constant in the model for the sentiment values. The auto-regressive component of the sentiment values for the cycle values is significant at a ten percent significance level, but not at a five percent significance level potentially indicating slight forecasting power. For the model for the sentiment values neither of the two auto-regressive components is significant at either a ten or five percent significance value. This will become a relevant detail when evaluating the forecasting performance of both models.
Lastly, the residuals of the cycle and sentiment values show a slight positive correlation.

At next, I will demonstrate the predictive performance of both models in three graphs.

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{../figures/ARIMA_forecasts_cycle.png}
\end{figure}
\newpage
\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{../figures/VAR_forecasts_cycle.png}
\end{figure}

In the two graphs above, one can observe training and test data of the cycle values plotted against the forecasts for both time-series models. As we can see the forecasts look quite similar with the ARMA model seemingly being better at recognizing the trend of the test data. From this plot we can infer that the addition of the sentiment values to the time-series analysis does not seem to lead to an immense improvement in forecasting performance and that rather the VAR model is actually slightly worse at forecasting the test data.

To understand the reasons for this, we will look at another plot.

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{../figures/VAR_forecasts_sentiment.png}
\end{figure}

In this plot we can see, the training and test data of the sentiment values plotted against the forecasts of the VAR model. As we can see the VAR model seems to not be able to identify the correct trend in the data. From this, we can infer that the forecasts for the sentiment values seem to get exponentially imprecise as we try to predict further into the future. Therefore, their conclusion in the forecasts for the cycle values becomes irrelevant.

Although these results are disappointing, there is still some evidence that the inclusion of sentiment values might improve forecast for cycle values in the very short-term. In a cross-validation analysis done on ten splits of the training data I found that the three-days-ahead forecasts of the VAR model outperformed the forecasts of the ARIMA model in eight of the ten splits. Therefore, for the very short-term the inclusion of sentiment values might lead to slightly better forecasting results.

The table containing the mean squared errors for these ten time-series splits can be found in the build folder of this project's repository.

\section{Discussion}

Lastly, I will talk about possible improvements for this project.
The improvements that probably would have had the biggest impact on the project would have been to improve the accuracy of the daily sentiment label. To do so I would have created more data to fine-tune the model with and also created a deep learning model that would calculate a weight for each headline of a specific day. This could improve the accuracy of the model immensely and thereby could also lead to an improvement of the forecasting performance of the VAR model. To create such a deep learning model one would have to label a dataset of headlines based on their perceived importance for the German economy and then trained the model on that dataset. One could have then used the label from this model together with the score that the sentiment analysis model provided when analyzing the headlines, lowering the weight of headlines with a low score and increasing the weight of headlines with a high score, to weight each individual headline of that specific day. Those two improvements would hopefully improve the performance of the daily sentiment label and could lead to an improvement in forecasting as well.
The second major improvement that I would have done is to improve the forecasts for the sentiment label values. To do so one would have to experiment with different approaches including more modern time-series forecasting models that, for example, rely on random forests and decision trees and even more additional variables to see if the sentiment label values can be forecasted more precisely.
The last possible improvement for this project would have been to use other sources of data. Using different representative sources for public sentiment as well as different indicators and indexes for economic activity could lead to other valuable insights. Nevertheless, due to the time restriction of this project, I was not able to extend my investigations in these directions and those investigations will have to follow in future work.


\setstretch{1}
\printbibliography
\setstretch{1.5}


% \appendix

% The chngctr package is needed for the following lines.
% \counterwithin{table}{section}
% \counterwithin{figure}{section}

\end{document}
